# Google Show & Tell for Android

This project uses Google's Show & Tell model to caption Android screenshots. To train in a 
reasonable amount of time you must use a GPU. We used the lab computer bg9 for this project. 
Instructions on how to run the code can be found 
[*here*](https://github.com/tensorflow/models/tree/master/research/im2txt.). Remember to 
## Setup
First, place Anaconda at the begining of your path.

export PATH=/opt/anaconda3/bin/:$PATH

The directory structure will be the same as in the Google tutorial except we do not use MSCOCO 
images. Directory /im2txt/mscoco from the tutorial will be replaced with /im2txt/Clarity. 
**Remember to alter the paths from the Google tutorial to fit your computer**
Several shell scripts have been added to the repository to make training and related activities
easier to run. The shell scripts set environmental variables so make sure to **Check the paths** 
hardcoded into the file before running a script.


## Preprocess
im2txt needs TFRecord files as input to the model. The preprocessing script (preprocess_im2txt.sh)
makes all of the TFRecord files and word_counts.txt. The script takes the directory that you want to save the TFRecords
to as input. 

./preprocess_im2txt.sh (path to TFRecord directory)


## Train

The training script is train_im2txt.sh. Training resumes from the last saves checkpoint, so if you want to train from
scratch you have to make sure that the MODEL_DIR in the script is set to an empty directory. The training script takes the number of
training steps as a parameter.

example

./train_im2txt.sh 1000000


## Inference 

The inference is inference_im2txt.sh. The script generates captions for one or multiple images. By default the script generates captions for every image in the 
data/Clarity/raw_data/val directory and saves it to coco-caption/results/captions_val_cap_results.json. However, you have the option 
of entering the file name or file pattern of your choice as an argument. It is important to note that this script **Only** generates the first caption generated my the model
for each image. 

examples 

#Activate virtual environment
source activate (name of environment)

#Run with the default value
./inference_im2txt.sh 

#Run on the test directory
./inference_im2txt.sh /Clarity/im2txt/data/Clarity/raw-data/test/image_id_* 

## BLEU Score Evaluation 

We use the evaluation script in the *coco-caption* directory (cocoEvalCaptions.py) to check Bleu scores. The script is 
written in python 2, so you will need to set up a conda virtual environment that uses Python 2.7 
and install the necessary dependencies in your environment(scipy and matplotlib). 

conda create -n (name of your environment) python=2.7 (create a conda virtual environment)

conda install -c conda-forge pycairo

conda install matplotlib

conda install scikit-image


After installing the above packages activate your environment with - source activate (name of environment)
 
The script uses two files to calculate the BLEU scores: 1. coco-caption/annotations/captions_*.json. The * can be 
train, test, or val, depending on what set you want to calculate. 2. coco-caption/results/captions_val_cap_results.json
this file is generated by the inference script.

By default the coco-caption code evaluates the validation set. To evaluate another set make sure to alter the inference script 
and cocoEvalCaptions.py accordingly. 

Now you can run the script 

cd coco-caption

python cocoEvalCaptions.py


## Evaluation

To run the evaluation shown in the im2txt documentation run:

./eval_im2txt.sh

It is a good idea to run this script in the background because it executes indefinitely
